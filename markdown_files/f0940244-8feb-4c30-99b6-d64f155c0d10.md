---
uuid: f0940244-8feb-4c30-99b6-d64f155c0d10
share: true
title: ETL to QE, Update 27, Meme Schema Roadmap to Implementation
---
[2024-02-15](../2024-02-15)

As a reminder the plan to MVP is,

TLDR; Memes, Schema, Tokens, [Merkle Tree](../7c574108-7671-4457-8f02-170d273cdbb6)s

1. Memes = [CGFS Meme Model](../88bdf6a2-d788-4352-bb46-373a72542d71)
2. Schema = [CGFS Persona Schema](../bbb2e4e9-08b9-461e-ba58-8a15c27d06d1)
3. Tokens = [QE - Token Specification](../a90fdbdd-c630-4c92-b79a-6dd2d68055b0)
4. [Merkle Tree](../7c574108-7671-4457-8f02-170d273cdbb6)s - [QE - Proof of Meme](../QE - Proof of Meme)

Okay so what is the ELI5 for the [CGFS Meme Model](../88bdf6a2-d788-4352-bb46-373a72542d71)

All messages have the same characteristics,

* Mentions
* Reply to message
* To Identities
* Time Sent
* Title
* Content Type
* Content
* Raw Tagging (Hashtags)
* Reactions
* Read Recites

Then there are complex messages features such as,

* Polls
* Encryption
* Pseudonym Management
	* Signal uses Contact List name
	* Discord Nicknames in Guilds
* Project Management
	* [Kan Board](../21e152ea-802c-4101-9910-90ca9a0b4a23)
	* [Issue Tracker](../6fa82807-8208-4439-85b5-81424067c4b3)
	* [Gantt Chart](../e4cec0b3-d3ef-4474-8c06-aba09e81e3d2)
	* [Checklists](../Checklists)
	* [Tier List](../a48f8577-e599-48d4-ad63-b0ac4980ff7d)
* Time Management (Calendaring), Time Table
* Forums / Questionnaires
* Threaded Conversations
* Flash Cards
* Ontology Tagging
* Tier List
* Algorithm Feed
* [Mind Map](../3e4abceb-b485-4ded-851e-c095080043a3)
* [CMS](../66ce9697-462e-45a1-befe-5330a52d1145) Graph
* [CRM](../7d9fa0af-e0be-4674-8fc2-380b641f2564) Graph
* [Annotation Software](../e05c991c-dfcc-463c-a05c-15867785d629)
* Moving messages between channels
* Custom Emoji's
* Payments
* Raw Logs ([systemd](../2bf93e44-adcb-4d5a-9d85-4d82015e6983))
* Slash Commands, gRPC
	* See [Mattermost](../f3a6ba74-0607-47a0-9394-a3cb82b65981), [Telegram](../0f693b68-2bb2-4679-8665-e25024c47a34), [Slack](../f509ed53-d79a-4030-8984-9a02088a04ef), and [Discord](../434d4a81-f2cc-4a50-b75c-0c66af4c15b2) for examples

Individual memes are atomic but memelets are not. A version controlled note, like this one, must be treated as a memelet rather than a meme.

The content must be separate from the timestamp.

The mentions must be stored as DIDs

Now all memes have a title, but some messaging systems require it.

Can the title be stored in the metadata?

Yes.

Can the timestamp be stored in the metadata?

Yes

Do we separate out the required metadata from the operational metadata?

The timestamp is part of the identity, that is Phase 2.

Alright a meme requires 3 components, a type, it's content, and metadata. THAT IS IT, everything else is optional.

Good now we need [JSONSchema](../ae47732c-10e8-4d3b-b365-9c3902febdfa) for that.

Ah we need one more thing, a version number. So we know what it is

We can encode a version number using the CID identifier

No we need it in the JSON

Do we just reserve the root key dd?

Sure, so we now have 4 things, a type, a version number, content, and a key value store.

Yes that sounds about right.

What keys do we use for these items?

* QEVersion : String
* type : Object
	* name : String
	* version : String
	* JSONSchema : JSON
* content : String
* data : Object

What does the JSONSchema look like?

``` JSON
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "CustomSchema",
  "type": "object",
  "properties": {
    "QEVersion": {
      "type": "string"
    },
    "type": {
      "type": "object",
      "properties": {
        "name": {
          "type": "string"
        },
        "version": {
          "type": "string"
        },
        "JSONSchema": {
          "type": "object",
          "additionalProperties": true
        }
      },
      "required": ["name", "version", "JSONSchema"]
    },
    "content": {
      "type": "string"
    },
    "data": {
      "type": "object",
      "additionalProperties": true
    }
  },
  "required": ["QEVersion", "type", "content", "data"]
}

```
[Source](https://chat.openai.com/c/8d5bb5c9-c19c-4348-923a-82dbf5aa4b24)

Nice now we need to standardize the other data, let's start with the DID's.

``` bash

did:dd:twitter:$USERNAME

did:dd:keybase:$USERNAME

did:dd:discord:$USERNAME:$USER_ID

did:dd:facebook:$PROFILE_ID

did:dd:instagram:$PROFILE_ID

did:dd:matrix:$MATRIX_ID

did:dd:nostr:$NOSTR_ID

did:dd:apub:$ACTIVITY_PUB_ID

did:dd:email:$EMAIL_ADDRESS

did:dd:hypothesis:$HYPOTHESIS_USERNAME

did:dd:phone:$PHONE_NUMBER_WITH_AREA_CODE

```


I just took a look inside the data [Facebook](../ffaff3fb-3237-40f8-abba-0b87ed05c16b) gives you when you request it. Turns out it does not include a link to the users Profile. Therefore in order to index everything we will require IPNS names.

Is requiring a private key for each identity a good idea.

It's either use a UUID or use an entire private key.

Ya I like using an entire private key.

So every person I would index would get their own private key just to index them.

Ya that sounds about right.

Okay now we need to come up with out types. 

I think the solution is to ETL each message type.

What do you mean.

Find better examples of each message type, then get a JSONSchema for each.

That sounds like a good idea.

Then we need to define how the JSON gets transformed.

Ask [ChatGPT](https://chat.openai.com/share/e925202c-60bf-47c2-8b6c-d73bbee94445)
Search "Can [jq](../6919cd5a-1a38-47e7-be2e-a3db232144dc) transform data?"
Result [How To Transform JSON Data with jq | DigitalOcean](https://www.digitalocean.com/community/tutorials/how-to-transform-json-data-with-jq)

Okay seems like jq will be all we need to get the data from whatever JSON we start with to the standardized format we like.

* Old - [Catechism - Personal Data Archive](../Catechism - Personal Data Archive)
* New - [Catechism - CGFS Meme Model](../f8a441e8-67b1-4672-9dad-a1ad8ed192a2)

Alright how we  we know where we want to be we need to articulate the path to get there.

We want to put all my messages in the same NDJSON meme format.

The raw messages will require a JSONSchema for their traditional message type, then a series of jq transformations in order to get it into what we need.

So does this mean we stop treating my personal data archive as a bunch of files I access via [mega-cmd](../6824a4cc-8501-42a2-8450-ef0d88d36f6b)?

That's a good question. The end goal here is to replace MEGA with a file system that supports hash based identifier, path mounting via [fuse](../78650c6f-ee5c-41fc-ab23-bb6fdce6d61e), encrypted sharing, shared folders, real time updates, etc. etc. etc.

Okay writing an entire file system or upgrading/using [PerKeep](../9c7ee4a4-18d0-452d-b707-cc2decd6b425) [Mintter](../92d347a2-b197-4c23-b247-e10bf6052ea7).

Actually [IPNS](../2bde5c00-e98d-4182-ac7f-5f7c24f0bd93) and [CID](../87e4fb9d-e5a8-4657-a7ba-f0962d1d075a)'s might be all we need except for the pubsub. A [CAR](../1cf53dcd-8555-48ce-8782-bba0d34c1f2a) file is a copy of what gets mounted to your file system right?

Okay what is the use case here?

I want to share a presentation that I edit with other people.

That's just not happening with [Libre Office](../876fe92e-04fb-4403-badd-fb72f36909a5). The file gets saved and passed around.

What about sharing a ebook library?

The ebook library should be [DAG-JSON](../542cf224-0a5f-4c62-b4f8-41521da2dd50) and link to CIDs. When someone does not have the specific book they request it from other people on the network.

Ah so we just need to resolve every CID in the DAG-JSON.

But that's not writing out to the file system.

That's just an implementation problem. A simple script that reads every author+title can then output to the file system.

What about my photo library?

That should be DAG-JSON as well except it should be chunked just like large chat logs.

What about my facebook exports and stuff?

Each export should be a CAR file and all the CAR files should be managed using DAG-JSON.

The key thing to understand here is that we want to now refer to the same file by its hash in multiple places and we can't currently do that.

Alright replacing [mega-cmd](../6824a4cc-8501-42a2-8450-ef0d88d36f6b) is out of scope, that can be resolved in the next step, Schema = [CGFS Persona Schema](../bbb2e4e9-08b9-461e-ba58-8a15c27d06d1). Oh also if I just dump [CID](../87e4fb9d-e5a8-4657-a7ba-f0962d1d075a)'s to the linux file system I can just symlink them where I want them whahahaha.

Alright let's take a look at where we want to be?
![Catechism - CGFS Meme Model](../f8a441e8-67b1-4672-9dad-a1ad8ed192a2)

Nice now what are the steps to get there.

1. Get better demo JSON Data for each of these platforms.
2. Define a JSONSchema for the OG data.
3. Save all these JSONSchemas somewhere and refer to them via CID for now.
4. Write ETL scripts for each data using [jq](../6919cd5a-1a38-47e7-be2e-a3db232144dc)
	1. WRITE DATA GENERATORS
	2. GET EXAMPLE DATA
	4. WRITE TESTS
5. Reindex all the data I posted above without the need for cryptographic DIDs
	1. Check if we can use a [Helia](../5d0d438a-16de-4cb5-8054-602f0847112d) datastore, check out [What is that IPFS S3 JS javascript Library?](../What is that IPFS S3 JS javascript Library?)
6. Upgrade JSONSchema to use Cryptographic DIDs rather than raw CIDs.

There is one part of this plan that does not sit right with me and that is the possible use of [Mutable File System](../2ec3ff8d-d30f-4ccb-9d11-2713a25c277b) and [IPFS UnixFS](../b6e5b09d-6901-4eb5-97c0-814384aa6e4a). These abstractions are separate from [Multiformats](../5ca2a82c-f00e-43eb-bc7e-7fdde4012434) that are not compatible with other programming languages. We assume with confidence that DAG-JSON and friends are a solid base to build up from. This MFS and UnixFS stuff doesn't seem as well supported.

[Can IPFS UnixFS in Javascript be exported to a CAR file?](../Can IPFS UnixFS in Javascript be exported to a CAR file?)

Okay so UnixFS is file and actually portable because CAR files can be extracted and dumped onto an actual UnixFS.

[What is the difference between IPFS MFS and UnixFS?](../What is the difference between IPFS MFS and UnixFS?)

Alright so MFS solves the problem of updating data. MFS updates the file system irrespective of Identity and context preservation.

Actually let's test exporting it using CAR.

Aaaand a couple hours later what do we got?

Seems like [IPFS IPLD CID Tutorial](../100d6889-e83d-4967-bec2-7e9424d8cd24) and [How to use Firebase to host IPFS static site?](../b6b253e7-c265-44b4-8436-2594a1f7e139) is going to have to be updated with UnixFS.

* [raw.githubusercontent.com/alanshaw/helia-unixfs-example/main/index.js](https://raw.githubusercontent.com/alanshaw/helia-unixfs-example/main/index.js)
* [Working with Content Archive (CAR) files ⁂ web3.storage](https://web3.storage/docs/concepts/car/#cars-and-web3storage)
* [dentropy/ipfs-cid-tutorial](https://github.com/dentropy/ipfs-cid-tutorial)
* [@ipld/unixfs - npm](https://www.npmjs.com/package/@ipld/unixfs)
* [ipfs-unixfs-importer - npm](https://www.npmjs.com/package/ipfs-unixfs-importer)
* [ipfs-car - npm](https://www.npmjs.com/package/ipfs-car)
* [ipfs-unixfs-exporter - npm](https://www.npmjs.com/package/ipfs-unixfs-exporter)
* [web3-storage/ipfs-car: 🚘 Convert files to content-addressable archives and back](https://github.com/web3-storage/ipfs-car)
* [Export Files as CAR](https://chat.openai.com/share/cecebf53-cbe0-49e9-a4b1-3a914e1944ad)
* [protobuf unixfs ipfs tutorial at DuckDuckGo](https://duckduckgo.com/?q=protobuf+unixfs+ipfs+tutorial&t=brave&ia=web)
* [The Road to UnixFS. Rust IPFS: Append-Only Log #002 | by Joonas Koivunen | Medium](https://medium.com/@koivunej/the-road-to-unixfs-f3cf5222b2ef)
* [helia-unixfs/packages/unixfs/test at main · ipfs/helia-unixfs · GitHub](https://github.getafreenode.com/ipfs/helia-unixfs/tree/main/packages/unixfs/test)
* [helia/packages/unixfs/test/mkdir.spec.ts at main · ipfs/helia](https://github.com/ipfs/helia/blob/main/packages/unixfs/test/mkdir.spec.ts)
* [IPFS Content Addressed Archiver](https://car.ipfs.io/)
* [ipfs-content-addressed-archiver/src/app.jsx at main · olizilla/ipfs-content-addressed-archiver](https://github.com/olizilla/ipfs-content-addressed-archiver/blob/main/src/app.jsx)

Okay let's define a goal here.

So [CGFS Persona Schema](../bbb2e4e9-08b9-461e-ba58-8a15c27d06d1) is going to be a series of [IndexedDB](../9fea8cfd-e8fa-4324-921c-e9455862e374) key value store. There does exist the [IPFS Stores](../c08e1848-c53e-4edc-9b11-e85f341fb7bc)

Should we care about unixfs in browser after, [this](https://github.com/ipfs/helia/issues/437) and [this](https://github.com/ipfs-examples/helia-examples/issues/294) happened.

What did we want to do with unixfs anyways?

I want to send users CAR files rather than trying to sync leveldb.

Car files have a problem because they can't be chunked or easily loaded into a browser. We don't want to pull a [memex.garden](../c6cc57b2-ed86-4f69-b656-c534988f6673) with the glitchy sync client.

Plus the entire idea of CAR files does not work S3, it may work for [Filebase](../d5f0f13e-5c6e-4019-b235-e7b316df6131) providing a path for each file within the CAR file but definitely not for S3, the CAR file would have to be extracted to S3. What we learned here is that CAR files are great in a Unix environment but suck in the browser.

UnixFS doesn't even work for the complex file permissions we want to add to things.

The best thing to do would be to open a web socket and have a conversation asking for everything a DID at a time, treating other people's levelDB as a file system. Not with SQL or something just raw levelDB.

Can DAG-JSON actually resolve using LevelDB?

Nope that's up to us to resolve that, that seems like code I can write. It should be simple once the user journey for all the data is defined.

Okay what now?

Next is get better examples of the JSON for each social media platform stuff in a git repo and explain how to transform it.

Also for Raindrop which is CSV we should just use sqlite or maybe duckdb. SQLite is definitely far more universal, I can just use a python script that uses native python. 

CAR files are like ZIP and TAR files they are for archiving not every day use. Also one can publish a CAR files to S3 by just extracting it and copying the directory over to S3.

Okay that's solved so what exactly is the problem?

But don't we want to do all the message format research in Obsidian and not in the repo itself?

Yes, what is the purpose of the repo then?

It is supposed to have the ETL procedures, the recursive jq commands to get the data into the required format.

Yes that's what we want.

Alright let the research project begin.

What about the previous research projects?

* [Research - Schema Comparisons](../Research - Schema Comparisons)
* [Filling self described SQL models via LLM Heilmer Catechism](../c1f5a29f-e664-480a-86c3-67efed75ff0b)

Alright all these projects each need their [Heilmeier Catechism](../edc84150-2be7-4533-8a4b-768eeff624af). They should not truly begin until that is complete in at least some form that you are willing to put on your public blog and point other people to.

Okay I like that requirement. And we have already started that,

[Catechism - CGFS Meme Model](../f8a441e8-67b1-4672-9dad-a1ad8ed192a2)

[How to export a Indexedb LevelDB database from an Electron App or website or extension?](../How to export a Indexedb LevelDB database from an Electron App or website or extension?)

That is way harder than it is supposed to be. It would literally be easier to hook up to a localhost daemon via websockets. 

[How to generate a PGP key with a seed in javascript?](../How to generate a PGP key with a seed in javascript?)

#### Can't Read Chromium IndexedDB

Wasted 6 hours looking into LevelDB what did we learn?

* Chrome uses a special version of LevelDB that you can't read without compiling it from Chrome itself or loading up chrome and trying to export the data
* There is no setting you can load into Chrome or Brave to force allowing loading scripts into sites see [Chromium --allow-running-insecure-content](../dbce8731-455a-414d-af09-d9c03f65a4e8)
* The best option is to load up [puppeteer](../123312b8-2afa-4828-aacc-50ba34f2f86e) or [Electron Software](../393cce93-b698-40cf-bfbb-51f0fcfc1734) with the appropriate the load custom javascript into [Chromium](../e9dcbb85-e010-4a16-970f-255045f0530d) and then save as JSON
	* Download JSON Files
	* Read Javascript directly from console
	* Load into some server
* [Firefox](../ad59b7e4-6f57-4b6e-b654-e982ebc765c4) stores their IndexedDB stuff in SQLite and it is hard to make sense of.

#### PGP in Javascript

We wasted many hours trying to figure out how to generate PGP keys in javascript.

* There are two PGP implementations
	* [openpgp js](../70086c2c-2b63-4270-b784-d2a495ce6da1)
	* [kbpgp](../e1512692-73d2-4bd8-a86f-59121d59eb4c)
* There is a tool to generate PGP keys with a seed
	* [ed25519-keygen - npm](https://www.npmjs.com/package/ed25519-keygen)
	* Dangit, I just solved my problem
* There is an alternative to PGP called [tweetnacl](../82318f38-4dec-4efa-b246-b4dff48813f2) that supports generation of keys from a seed.
* [RSA](../f04a63f2-68b0-49cc-9fef-b93dee955a09) Stuff
	* [Is there a way to generate RSA keys from a seed?](../Is there a way to generate RSA keys from a seed?)
	* [Is there a way to convert a RSA key to a SSH or PGP javascript?](../Is there a way to convert a RSA key to a SSH or PGP javascript?)
	* [Is there a way to extract a RSA key from a PGP key in javascript?](../Is there a way to extract a RSA key from a PGP key in javascript?)
	* [Do any blockchains use RSA?](../Do any blockchains use RSA?)

Cool we have partially solved our PGP in JS problem and IPNS keygen problem at the same time.

#### Missing User Journey

Alright I feel like we are still missing a user journey......

Well we want to develop our own Nostr client but be able to play with the data client side [Obsidian](../f76a085e-f2c8-43bd-a852-47760f01e401), SQL, drag and drop, file system style.

How would your mother use this?

My mother emails me stuff and keeps notes in her email. The stuff she would normally email me she is supposed to send me via QE. Then she and I can integrate all those messages into a wiki with context.

How are my friends supposed to use this?

Every person in the group chat now has a wiki page to themselves as well as a collaborative wiki.

This wiki part is not in your pitch deck from the other day.

Yes that will need to be fixed, added to the backlog.

Okay so what can we get done this weekend?

Develop my own Nostr client with React.

Support,

* Event 0
	* Pseudonym / Username
	* Description
	* Bitcoin Lightning Address
* Pseudonym / Username
* Description

What else did we learn about?

#### Interrogate CAR Files

So manually creating a CAR file in browser even with all the examples was a bitch and a half.

I was thinking that CAR files could basically be mounted in a IndexedDB name space. Then I could manipulate them with the same old [mv](../091d68a4-99bd-4cf8-9271-af5716788d1d), [cp](../7ae281a4-1bd6-49d5-8ecc-dd4234be7f65), [ls](../49e56a94-0408-45fa-88b8-a010266954f7) commands I am used to to.

CAR Files verses Custom IPLD Structure.

So turns out that CAR files are just IPLD protobuf storing binary blobs at the end attached to file names.

#### Sources

* [did-pkh/did-pkh-method-draft.md at main · w3c-ccg/did-pkh](https://github.com/w3c-ccg/did-pkh/blob/main/did-pkh-method-draft.md)

#### Backlinks

* [Project Update Posts](/4c45797f-8d43-4277-a5c1-de8df9aa7876)
* [ETL to QE, Update 28, Separation of Concerns](/1c28c038-689a-4083-a472-3bdab8489c4f)