---
uuid: adf51542-a86b-437b-8542-9ef82c41d7a2
share: true
title: ETL to Question Engine, Update 1
---
Date: [2023-10-02](/undefined)

## Removing SQLite from ETL Pipeline

Today I practised running my ETL pipeline from end to end. During this process going from json files to sqlite to postgres is not that ideal. The reasons for converting to sqlite in the middle were the following,

1. It is easier to load in a [sqlite](/1a1ccc57-1ba3-4ba7-8db9-9eb945b88d85) file [jupyter](/14b19809-58b0-44c8-a719-c50badebb08c) notebooks than host and connect to a [Postgres](/5d70cd64-3134-4b62-8879-12f1f8bb4afe) database
2. One can only load the particular discord guild's they want analytics on into Postgres any of a list of other [Data Engineering Tools](/undefined)
3. It is much easier to share a [sqlite](/1a1ccc57-1ba3-4ba7-8db9-9eb945b88d85) file with someone else and start using it than a dump from [Postgres](/5d70cd64-3134-4b62-8879-12f1f8bb4afe)

The extra step in the procedure was making things much more complicated than they needed to for example here are a list of problems I had,

1. Conversion of type timestamp from [sqlite](/1a1ccc57-1ba3-4ba7-8db9-9eb945b88d85) to [Postgres](/5d70cd64-3134-4b62-8879-12f1f8bb4afe) did not work without first copying the CSV to a postgres table, creating additional columns in said table of type timestamp, then casting the unix_time time stamp from sqlite
2. There is an issue of which database get's to load the raw data. Transformations were being done inside SQLite  before getting put into postgres making the data in postgres dependent on sqlite when it may be better to load the json files directly into postgres.
3. Each discord guild had a separate SQLite database therefore if one wanted to do analysis on two of them at the same time they had to wither write complex connecting code or merge the SQLite databases together. Merging the SQLite databases together is possible though a hassel I did not want to deal with. I realized I can just put all the data into postgres and output whatever data I want to SQLite if I really wanted to.
4. I learned about [DuckDB](/undefined) as an alternative to SQLite, DuckDB has a built in [Postgres connector](https://duckdb.org/docs/extensions/postgres_scanner.html) so I expect creating a portable dataset from Postgres into DubkDB will be much easier than SQLite. DuckDB can even read [Parquet files](https://duckdb.org/docs/data/parquet/overview.html), a file format I should probably be using and I will be coming back to....

## Experience Running ETL pipeline on M1 Macbook

On my desktop I have about 10,000,000 messages loaded into my Postgres database. I got the same raw JSON files and ran the same script to load the same messages into a Postgres database on my M1 Macbook. After about an hour and a half I had only loaded about 2.5 million messages and I candled the rest of the data transformation because I had enough data in the database to perform analytics. This was taking a lot longer than I expected, for example here are some thoughts I had.

1. SQLite running on Disk took much longer than loading into Postgres
2. SQLite running in memory and saving to disk was ridiculously fast taking about an hour to load everything, of course each guild has their own database rather than having a single database with everything
3. Migrating the data from SQLite to Postgres was faster than loading in the raw JSON data I expect this is because of a couple different factors.
	1. I am loading data into JSON table now native SQL tables with proper types
	2. The batches committed to Postgres in the SQLite transformation were larger allowing for postgres to optimize it's inserts
	3. Postgres is writing everything to Disk, maybe there are settings I can set, like giving it more ram, what will speed up this process